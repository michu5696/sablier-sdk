"""Model class representing a Sablier model"""

from typing import Optional, Any, List, Dict
from ..http_client import HTTPClient
from ..workflow import WorkflowValidator, WorkflowConflict


class Model:
    """
    Represents a Sablier model
    
    A model encapsulates the entire workflow:
    - Feature selection
    - Data fetching and processing
    - Sample generation
    - Encoding model fitting
    - QRF training
    - Forecasting
    """
    
    def __init__(self, http_client: HTTPClient, model_data: dict, interactive: bool = True):
        """
        Initialize Model instance
        
        Args:
            http_client: HTTP client for API requests
            model_data: Model data from API
            interactive: Whether to prompt for confirmations (default: True)
        """
        self.http = http_client
        self._data = model_data
        self.id = model_data.get('id')
        self.name = model_data.get('name')
        self.description = model_data.get('description', '')
        self.interactive = interactive
    
    def __repr__(self) -> str:
        return f"Model(id='{self.id}', name='{self.name}', status='{self.status}')"
    
    # ============================================
    # PROPERTIES
    # ============================================
    
    def data_collection(self, fred_api_key: Optional[str] = None):
        """
        Create a DataCollection instance for this model
        
        Args:
            fred_api_key: Optional FRED API key for searching and fetching
            
        Returns:
            DataCollection: Data collection instance scoped to this model
            
        Example:
            >>> data = model.data_collection(fred_api_key="...")
            >>> data.search("treasury")
            >>> data.add("DGS10", source="FRED")
            >>> data.fetch_and_process()
        """
        from ..data_collection import DataCollection
        return DataCollection(self, fred_api_key=fred_api_key)
    
    # ============================================
    # PROPERTIES (continued)
    # ============================================
    
    @property
    def status(self) -> str:
        """Get current model status"""
        return self._data.get('status', 'created')
    
    @property
    def input_features(self) -> List[Dict[str, Any]]:
        """Get input features"""
        return self._data.get('input_features', [])
    
    def refresh(self):
        """Refresh model data from API"""
        response = self.http.get(f'/api/v1/models/{self.id}')
        self._data = response.get('model', {})
        return self
    
    def delete(self, confirm: Optional[bool] = None) -> Dict[str, Any]:
        """
        Delete this model and ALL associated data
        
        This will permanently delete:
        - Model record
        - Training data
        - Generated samples
        - Encoding models
        - Trained QRF model (from storage)
        - All scenarios using this model
        
        Args:
            confirm: Explicit confirmation (None = prompt if interactive)
            
        Returns:
            dict: Deletion status
            
        Example:
            >>> model.delete()  # Will prompt for confirmation
            >>> model.delete(confirm=True)  # Skip confirmation
        """
        # Always warn for deletion
        print("âš ï¸  WARNING: You are about to PERMANENTLY DELETE this model.")
        print(f"   Model: {self.name} ({self.id})")
        print(f"   Status: {self.status}")
        print()
        print("This will delete ALL associated data:")
        print("  - Training data")
        print("  - Generated samples")
        print("  - Encoding models")
        print("  - Trained QRF model")
        print("  - All scenarios using this model")
        print()
        print("This action CANNOT be undone.")
        print()
        
        # Get confirmation
        if confirm is None and self.interactive:
            response = input("Type the model name to confirm deletion: ")
            confirm = response == self.name
            if not confirm:
                print("âŒ Model name doesn't match. Deletion cancelled.")
                return {"status": "cancelled"}
        elif confirm is None:
            # Non-interactive without explicit confirm
            print("âŒ Deletion cancelled (interactive=False, no confirmation)")
            return {"status": "cancelled"}
        elif not confirm:
            print("âŒ Deletion cancelled")
            return {"status": "cancelled"}
        
        # Delete via API
        print("ðŸ—‘ï¸  Deleting model...")
        response = self.http.delete(f'/api/v1/models/{self.id}')
        
        print(f"âœ… Model '{self.name}' deleted successfully")
        
        return response
    
    # ============================================
    # WORKFLOW VALIDATION
    # ============================================
    
    def _check_and_handle_conflict(self, operation: str, confirm: Optional[bool] = None) -> bool:
        """
        Check for workflow conflict and handle it
        
        Args:
            operation: Operation name
            confirm: Explicit confirmation (None = prompt if interactive)
            
        Returns:
            True if operation should proceed, False if cancelled
        """
        conflict = WorkflowValidator.check_conflict(operation, self.status)
        
        if not conflict:
            # No conflict, proceed
            return True
        
        # Conflict detected
        print(conflict.format_warning())
        
        # Get confirmation
        if confirm is None and self.interactive:
            response = input("\nContinue? [y/N]: ")
            confirm = response.lower() == 'y'
        elif confirm is None:
            # Non-interactive mode without explicit confirm, cancel
            print("âŒ Operation cancelled (interactive=False, no confirmation provided)")
            return False
        
        if not confirm:
            print("âŒ Operation cancelled")
            return False
        
        # User confirmed, proceed with cleanup
        print("ðŸ—‘ï¸  Cleaning up dependent data...")
        self._cleanup_dependent_data(conflict.items_to_delete)
        
        return True
    
    def _cleanup_dependent_data(self, items_to_delete: List[str]):
        """
        Clean up dependent data before operation
        
        Args:
            items_to_delete: List of items to delete
        """
        client = self.http
        
        for item in items_to_delete:
            try:
                if item == "training_data":
                    # Delete all training_data for this model
                    # Note: We use the backend's Supabase client, not direct deletion
                    print(f"  - Deleting training data...")
                    # Will be handled by regenerating samples
                    
                elif item == "samples":
                    # Delete all samples for this model
                    print(f"  - Deleting samples...")
                    # Cascade delete via database FKs
                    
                elif item == "encoding_models":
                    # Delete all encoding models for this model
                    print(f"  - Deleting encoding models...")
                    # Cascade delete via database FKs
                    
                elif item == "trained_model":
                    # Delete trained model from storage
                    print(f"  - Deleting trained model from storage...")
                    # Handled by model deletion if model_path exists
                    
                elif item == "feature_importance":
                    # Feature importance is in model_metadata
                    print(f"  - Clearing feature importance...")
                    # Will be overwritten on next training
                    
            except Exception as e:
                print(f"    âš ï¸  Warning: Failed to delete {item}: {e}")
        
        print("âœ… Cleanup complete (dependent data will be overwritten)")
    
    # ============================================
    # CONFIGURATION METHODS
    # ============================================
    
    def add_features(self, features: List[Dict[str, Any]], confirm: Optional[bool] = None) -> 'Model':
        """
        Add features to the model
        
        Args:
            features: List of feature configs, e.g. [{"name": "Gold Price", "source": "yahoo", "symbol": "GC=F"}]
            confirm: Explicit confirmation (None = prompt if needed)
            
        Returns:
            self for method chaining
        """
        # Check for conflicts
        if not self._check_and_handle_conflict("add_features", confirm):
            return self
        
        print(f"[Model {self.name}] Adding {len(features)} features...")
        
        # Get current features
        current_features = self.input_features.copy()
        
        # Add new features
        for feature in features:
            # Check if feature already exists
            if any(f.get('name') == feature.get('name') for f in current_features):
                print(f"  âš ï¸  Feature '{feature.get('name')}' already exists, skipping")
                continue
            current_features.append(feature)
            print(f"  âœ… Added '{feature.get('name')}'")
        
        # Update model
        response = self.http.patch(f'/api/v1/models/{self.id}', {
            'input_features': current_features
        })
        
        self._data = response.get('model', {})
        print(f"âœ… Features updated ({len(current_features)} total)")
        return self
    
    def set_training_period(self, start_date: str, end_date: str, confirm: Optional[bool] = None) -> 'Model':
        """
        Set training period
        
        Args:
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)
            confirm: Explicit confirmation (None = prompt if needed)
            
        Returns:
            self for method chaining
        """
        # Check for conflicts
        if not self._check_and_handle_conflict("set_training_period", confirm):
            return self
        
        # Validate minimum period length
        from datetime import datetime
        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
        total_days = (end - start).days
        
        MIN_TRAINING_DAYS = 180  # At least 6 months for meaningful models
        if total_days < MIN_TRAINING_DAYS:
            raise ValueError(
                f"Training period too short! Got {total_days} days, need at least {MIN_TRAINING_DAYS} days "
                f"(~6 months minimum for meaningful time series models)"
            )
        
        print(f"[Model {self.name}] Setting training period: {start_date} to {end_date} ({total_days} days)")
        
        # Update model
        response = self.http.patch(f'/api/v1/models/{self.id}', {
            'training_start_date': start_date,
            'training_end_date': end_date
        })
        
        self._data = response.get('model', {})
        print("âœ… Training period updated")
        return self
    
    # ============================================
    # DATA FETCHING
    # ============================================
    
    def fetch_data(
        self, 
        max_gap_days: int = 7,
        interpolation_method: str = "linear",
        fred_api_key: Optional[str] = None,
        confirm: Optional[bool] = None
    ) -> Dict[str, Any]:
        """
        Fetch and process training data from FRED and Yahoo Finance
        
        This method handles EVERYTHING:
        1. Fetches raw data from APIs
        2. Applies interpolation with specified gap limit
        3. Saves both raw and processed data to database
        4. Updates model status to 'data_collected'
        
        Args:
            max_gap_days: Maximum gap (in days) to fill via interpolation (default: 7)
            interpolation_method: "linear", "forward_fill", or "backward_fill" (default: "linear")
            confirm: Explicit confirmation (None = prompt if needed)
            
        Returns:
            dict: Fetch statistics with keys: status, features_fetched, total_raw_points, total_processed_points
            
        Example:
            >>> # Simple - just fetch with default 7-day linear interpolation
            >>> model.fetch_data()
            
            >>> # Custom - 30-day forward fill
            >>> model.fetch_data(max_gap_days=30, interpolation_method="forward_fill")
        
        Note:
            FRED API key is taken from the client initialization.
            Yahoo Finance doesn't require an API key.
        """
        # Check for conflicts
        if not self._check_and_handle_conflict("fetch_data", confirm):
            return {"status": "cancelled"}
        
        print(f"[Model {self.name}] Fetching data...")
        
        # Get features and training period from model
        features = self.input_features
        if not features:
            print("âŒ No features configured. Call model.add_features() first.")
            return {"status": "error", "message": "No features configured"}
        
        training_period = self._data.get('training_start_date'), self._data.get('training_end_date')
        if not all(training_period):
            print("âŒ Training period not set. Call model.set_training_period() first.")
            return {"status": "error", "message": "Training period not set"}
        
        # Build processing config - apply same max_gap to all features
        processing_config = {
            "interpolation": {
                "method": interpolation_method,
                "maxGapLength": {
                    feature.get("name", feature.get("display_name", "")): max_gap_days
                    for feature in features
                }
            }
        }
        
        # Build request payload
        payload = {
            "model_id": self.id,
            "features": features,
            "start_date": training_period[0],
            "end_date": training_period[1],
            "fred_api_key": fred_api_key,
            "processing_config": processing_config
        }
        
        print(f"  Features: {len(features)}")
        print(f"  Period: {training_period[0]} to {training_period[1]}")
        print(f"  Interpolation: {interpolation_method} (max {max_gap_days} day gaps)")
        
        # Call backend
        print("ðŸ“¡ Fetching from APIs and processing...")
        response = self.http.post('/api/v1/data/fetch', payload)
        
        # Update model status
        self._data["status"] = "data_collected"
        
        print(f"âœ… Fetched {response.get('total_raw_points', 0)} raw points")
        print(f"âœ… Processed {response.get('total_processed_points', 0)} interpolated points")
        
        return response
    
    # ============================================
    # SAMPLE GENERATION
    # ============================================
    
    def generate_samples(
        self,
        past_window: int,
        future_window: int,
        target_features: List[str],
        stride: int = 10,
        conditioning_features: Optional[List[str]] = None,
        splits: Optional[Dict[str, Dict[str, str]]] = None,
        confirm: Optional[bool] = None
    ) -> Dict[str, Any]:
        """
        Generate training samples from processed data
        
        Args:
            past_window: Past window size (days)
            future_window: Future window size (days)
            target_features: Features to predict (REQUIRED, must be subset of input features)
            stride: Stride between samples (days, default: 10)
            conditioning_features: Features for conditioning (optional, defaults to all non-target features)
            splits: Train/validation/test date ranges (optional, auto-calculated if not provided)
                Example: {
                    "training": {"start": "2020-01-01", "end": "2023-12-31"},
                    "validation": {"start": "2024-01-01", "end": "2024-06-30"},
                    "test": {"start": "2024-07-01", "end": "2024-12-31"}
                }
            confirm: Explicit confirmation (None = prompt if needed)
            
        Returns:
            dict: Generation statistics with keys: status, samples_generated, split_counts
            
        Example:
            >>> model.generate_samples(
            ...     past_window=90,
            ...     future_window=30,
            ...     target_features=["Gold Price"],
            ...     stride=10
            ... )
        """
        # Check for conflicts
        if not self._check_and_handle_conflict("generate_samples", confirm):
            return {"status": "cancelled"}
        
        print(f"[Model {self.name}] Generating samples...")
        print(f"  Past window: {past_window} days")
        print(f"  Future window: {future_window} days")
        print(f"  Stride: {stride} days")
        
        # Validate inputs
        self._validate_sample_generation_inputs(
            past_window, 
            future_window, 
            target_features, 
            conditioning_features, 
            splits
        )
        
        # Auto-assign conditioning features if not provided
        all_feature_names = [f.get('display_name', f.get('name')) for f in self.input_features]
        if conditioning_features is None:
            conditioning_features = [f for f in all_feature_names if f not in target_features]
            print(f"  Auto-assigned {len(conditioning_features)} conditioning features")
        
        # Auto-generate splits if not provided
        if splits is None:
            splits = self._auto_generate_splits()
            print(f"  Auto-generated splits from training period")
        
        # Validate splits
        self._validate_splits(splits, past_window, future_window)
        
        # Build sample config
        sample_config = {
            "pastWindow": past_window,
            "futureWindow": future_window,
            "stride": stride,
            "splits": splits,
            "conditioningFeatures": conditioning_features,
            "targetFeatures": target_features
        }
        
        # Build request payload
        payload = {
            "user_id": self._data.get("user_id"),  # For backend
            "model_id": self.id,
            "sample_config": sample_config
        }
        
        # Call backend
        print("ðŸ“¡ Calling backend to generate samples...")
        response = self.http.post('/api/v1/ml/generate-samples', payload)
        
        # Update model status
        self._data["status"] = "samples_generated"
        self._data["sample_config"] = sample_config
        
        # Update input_features with types
        self._update_feature_types(conditioning_features, target_features)
        
        split_counts = response.get('split_counts', {})
        print(f"âœ… Generated {response.get('samples_generated', 0)} samples")
        print(f"   Training: {split_counts.get('training', 0)}")
        print(f"   Validation: {split_counts.get('validation', 0)}")
        print(f"   Test: {split_counts.get('test', 0)}")
        
        return response
    
    def _validate_sample_generation_inputs(
        self, 
        past_window: int, 
        future_window: int,
        target_features: List[str],
        conditioning_features: Optional[List[str]],
        splits: Optional[Dict[str, Dict[str, str]]]
    ):
        """Validate sample generation inputs"""
        from datetime import datetime, timedelta
        
        # 0. Check minimum window sizes
        MIN_PAST_WINDOW = 30  # At least 30 days of history
        MIN_FUTURE_WINDOW = 5  # At least 5 days to predict
        
        if past_window < MIN_PAST_WINDOW:
            raise ValueError(f"past_window must be at least {MIN_PAST_WINDOW} days (got {past_window})")
        
        if future_window < MIN_FUTURE_WINDOW:
            raise ValueError(f"future_window must be at least {MIN_FUTURE_WINDOW} days (got {future_window})")
        
        # 1. Check we have input features
        if not self.input_features:
            raise ValueError("No features configured. Call model.add_features() first or use DataCollection")
        
        all_feature_names = [f.get('display_name', f.get('name')) for f in self.input_features]
        
        # 2. Target features must be specified and valid
        if not target_features:
            raise ValueError("target_features is required. Specify which features to predict.")
        
        invalid_targets = [f for f in target_features if f not in all_feature_names]
        if invalid_targets:
            raise ValueError(f"Invalid target features: {invalid_targets}. Available: {all_feature_names}")
        
        # 3. Conditioning features must be valid
        if conditioning_features:
            invalid_conditioning = [f for f in conditioning_features if f not in all_feature_names]
            if invalid_conditioning:
                raise ValueError(f"Invalid conditioning features: {invalid_conditioning}. Available: {all_feature_names}")
            
            # Check for overlap
            overlap = set(target_features) & set(conditioning_features)
            if overlap:
                raise ValueError(f"Features cannot be both target and conditioning: {overlap}")
        
        # 4. All features must be assigned a type
        assigned_features = set(target_features)
        if conditioning_features:
            assigned_features.update(conditioning_features)
        else:
            assigned_features.update([f for f in all_feature_names if f not in target_features])
        
        unassigned = set(all_feature_names) - assigned_features
        if unassigned:
            raise ValueError(f"All features must be assigned as target or conditioning. Unassigned: {unassigned}")
    
    def _validate_splits(self, splits: Dict[str, Dict[str, str]], past_window: int, future_window: int):
        """Validate split date ranges and check for overlaps"""
        from datetime import datetime, timedelta
        
        required_splits = ["training", "validation", "test"]
        for split_name in required_splits:
            if split_name not in splits:
                raise ValueError(f"Missing required split: {split_name}")
            if "start" not in splits[split_name] or "end" not in splits[split_name]:
                raise ValueError(f"Split '{split_name}' must have 'start' and 'end' dates")
        
        # Parse dates
        train_start = datetime.strptime(splits["training"]["start"], "%Y-%m-%d")
        train_end = datetime.strptime(splits["training"]["end"], "%Y-%m-%d")
        val_start = datetime.strptime(splits["validation"]["start"], "%Y-%m-%d")
        val_end = datetime.strptime(splits["validation"]["end"], "%Y-%m-%d")
        test_start = datetime.strptime(splits["test"]["start"], "%Y-%m-%d")
        test_end = datetime.strptime(splits["test"]["end"], "%Y-%m-%d")
        
        # Check minimum split lengths
        sample_size = past_window + future_window
        MIN_SAMPLES_PER_SPLIT = 10  # At least 10 samples per split
        min_split_days = sample_size + (MIN_SAMPLES_PER_SPLIT * 10)  # Assuming stride ~10 days
        
        train_days = (train_end - train_start).days
        val_days = (val_end - val_start).days
        test_days = (test_end - test_start).days
        
        if train_days < min_split_days:
            raise ValueError(
                f"Training split too short! Got {train_days} days, need at least {min_split_days} days "
                f"(sample size {sample_size} + room for ~{MIN_SAMPLES_PER_SPLIT} samples)"
            )
        
        if val_days < sample_size:
            raise ValueError(f"Validation split too short! Got {val_days} days, need at least {sample_size} days (sample size)")
        
        if test_days < sample_size:
            raise ValueError(f"Test split too short! Got {test_days} days, need at least {sample_size} days (sample size)")
        
        # Check temporal ordering
        if not (train_start < train_end < val_start < val_end < test_start < test_end):
            raise ValueError("Splits must be temporally ordered: training -> validation -> test (no overlaps)")
        
        # Check for date range overlaps
        if train_end >= val_start:
            raise ValueError(f"Training end ({train_end.date()}) must be before validation start ({val_start.date()})")
        
        if val_end >= test_start:
            raise ValueError(f"Validation end ({val_end.date()}) must be before test start ({test_start.date()})")
        
        # Check for sample overlap (critical!)
        sample_size = past_window + future_window
        
        # Gap between training and validation
        train_val_gap = (val_start - train_end).days
        if train_val_gap < sample_size:
            raise ValueError(
                f"Training and validation splits are too close! "
                f"Gap: {train_val_gap} days, Sample size: {sample_size} days. "
                f"A sample starting at training end could overlap into validation. "
                f"Minimum gap required: {sample_size} days"
            )
        
        # Gap between validation and test (warning only, contiguous is OK)
        val_test_gap = (test_start - val_end).days
        if val_test_gap < sample_size:
            print(f"  âš ï¸  Validation and test are close (gap: {val_test_gap} days, sample: {sample_size} days)")
            print(f"      Last validation samples may overlap into test period (acceptable)")
        
        print(f"  âœ“ Splits validated: no data leakage")
    
    def _auto_generate_splits(self) -> Dict[str, Dict[str, str]]:
        """Auto-generate splits from training period (70/20/10)"""
        from datetime import datetime, timedelta
        
        start = self._data.get('training_start_date')
        end = self._data.get('training_end_date')
        
        if not start or not end:
            raise ValueError("Training period not set. Call set_training_period() first")
        
        start_date = datetime.strptime(start, "%Y-%m-%d")
        end_date = datetime.strptime(end, "%Y-%m-%d")
        total_days = (end_date - start_date).days
        
        # 70/20/10 split
        train_days = int(total_days * 0.7)
        val_days = int(total_days * 0.2)
        
        train_end = start_date + timedelta(days=train_days)
        val_end = train_end + timedelta(days=val_days)
        
        return {
            "training": {
                "start": start_date.strftime("%Y-%m-%d"),
                "end": train_end.strftime("%Y-%m-%d")
            },
            "validation": {
                "start": train_end.strftime("%Y-%m-%d"),
                "end": val_end.strftime("%Y-%m-%d")
            },
            "test": {
                "start": val_end.strftime("%Y-%m-%d"),
                "end": end_date.strftime("%Y-%m-%d")
            }
        }
    
    def _update_feature_types(self, conditioning_features: List[str], target_features: List[str]):
        """Update input_features with type assignment"""
        updated_features = []
        for feature in self.input_features:
            feature_name = feature.get('display_name', feature.get('name'))
            feature_copy = feature.copy()
            
            if feature_name in target_features:
                feature_copy['type'] = 'target'
            elif feature_name in conditioning_features:
                feature_copy['type'] = 'conditioning'
            
            updated_features.append(feature_copy)
        
        # Update via API
        self.http.patch(f'/api/v1/models/{self.id}', {
            'input_features': updated_features
        })
        
        # Update local cache
        self._data['input_features'] = updated_features
    
    # ============================================
    # ENCODING
    # ============================================
    
    def encode_samples(self, confirm: Optional[bool] = None) -> Dict[str, Any]:
        """
        Fit PCA-ICA encoding models and encode all samples
        
        This method performs two operations:
        1. Fits encoding models for each feature-window combination
        2. Encodes all samples using the fitted models
        
        Args:
            confirm: Explicit confirmation (None = prompt if needed)
            
        Returns:
            dict: Encoding statistics with keys: status, models_fitted, samples_encoded
            
        Example:
            >>> result = model.encode_samples()
            >>> print(f"Fitted {result['models_fitted']} encoding models")
            >>> print(f"Encoded {result['samples_encoded']} samples")
        """
        # Check for conflicts
        if not self._check_and_handle_conflict("encode_samples", confirm):
            return {"status": "cancelled"}
        
        print(f"[Model {self.name}] Encoding samples...")
        
        # Step 1: Fit encoding models
        print("  Step 1/2: Fitting PCA-ICA encoding models...")
        fit_response = self.http.post('/api/v1/ml/fit', {
            "model_id": self.id,
            "user_id": self._data.get("user_id")  # For API key auth
        })
        
        models_fitted = fit_response.get('models_fitted', 0)
        print(f"  âœ… Fitted {models_fitted} encoding models")
        
        # Step 2: Encode samples
        print("  Step 2/2: Encoding all samples...")
        encode_response = self.http.post('/api/v1/ml/encode', {
            "model_id": self.id,
            "user_id": self._data.get("user_id")  # For API key auth
        })
        
        samples_encoded = encode_response.get('samples_encoded', 0)
        print(f"  âœ… Encoded {samples_encoded} samples")
        
        # Update model status
        self._data["status"] = "samples_encoded"
        
        print(f"âœ… Encoding complete")
        
        return {
            "status": "success",
            "models_fitted": models_fitted,
            "samples_encoded": samples_encoded
        }
    
    # ============================================
    # TRAINING
    # ============================================
    
    def train(self, qrf_config: Optional[dict] = None, confirm: Optional[bool] = None) -> 'TrainingResults':
        """
        Train the QRF model
        
        Args:
            qrf_config: Optional QRF configuration
            confirm: Explicit confirmation (None = prompt if needed)
            
        Returns:
            TrainingResults: Training results with metrics
        """
        # Training doesn't have conflicts (always replaces previous model)
        # TODO: Implement model training
        print(f"[Model {self.name}] Training model...")
        from .training_results import TrainingResults
        return TrainingResults({})
    
    # ============================================
    # FORECASTING
    # ============================================
    
    def generate_forecast(
        self,
        sample_id: str,
        n_samples: int = 100,
        conditioning_features: list[str] = None
    ) -> 'SyntheticData':
        """
        Generate forecast from a sample
        
        Args:
            sample_id: Sample ID to use for conditioning
            n_samples: Number of forecast samples to generate
            conditioning_features: Features to condition on
            
        Returns:
            SyntheticData: Synthetic data instance with forecasts
        """
        # TODO: Implement forecasting
        print(f"[Model {self.name}] Generating forecast...")
        from ..synthetic_data import SyntheticData
        import pandas as pd
        return SyntheticData(
            data=pd.DataFrame(),
            source_model=self,
            metadata={"n_samples": n_samples}
        )